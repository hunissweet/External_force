{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9accd6-d6c9-4e62-be8c-67f03bc22c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import LogNorm\n",
    "import sklearn.preprocessing\n",
    "#from directory_tree import display_tree\n",
    "# Customed Library\n",
    "import engine ,model_builder,utils\n",
    "import numpy as np\n",
    "#import torchinfo\n",
    "from timeit import default_timer as timer \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "#display_tree('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da9b79-41ac-4743-a5fa-8c5b4bb005a4",
   "metadata": {},
   "source": [
    "# 1 . load the function\n",
    "### load_data\n",
    "* input : pick\n",
    "* output\n",
    "\n",
    "### calibration_function\n",
    "* input : pickle of data. \n",
    "* output : two figures(flag- digital signal,F/T, Pressure, TCP, sensors data)\n",
    "\n",
    "### rawdataplot\n",
    "* input : pickle of data. \n",
    "* output : two figures(flag- digital signal,F/T, Pressure, TCP, sensors data)\n",
    "\n",
    "### rmse\n",
    "* input\n",
    "* output\n",
    "\n",
    "### plot_prediction\n",
    "* input\n",
    "* output\n",
    "\n",
    "\n",
    "### plot_loss_curves\n",
    "* input\n",
    "* output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7eb7cf-8d50-42e6-83b6-7a7a84f2ce52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#P_Sensor_col=Total_dataset.columns[9:21]\n",
    "def load_data(path):\n",
    "    Columns=['Flag',\n",
    "             'Force_X','Force_Y','Force_Z','Torque_X','Torque_Y','Torque_Z',\n",
    "             'Input_P_1','Input_P_2',\n",
    "             'TCP_X','TCP_Y','TCP_Z',\n",
    "             'Sensor_P_1_1','Sensor_P_1_2','Sensor_P_1_3','Sensor_P_1_4',\n",
    "             'Sensor_P_1_5','Sensor_P_1_6','Sensor_P_1_7','Sensor_P_1_8',\n",
    "             'Sensor_P_1_9','Sensor_P_1_10','Sensor_P_1_11','Sensor_P_1_12',\n",
    "             'Sensor_S_1',\n",
    "             'Sensor_P_2_1','Sensor_P_2_2','Sensor_P_2_3','Sensor_P_2_4',\n",
    "             'Sensor_P_2_5','Sensor_P_2_6','Sensor_P_2_7','Sensor_P_2_8',\n",
    "             'Sensor_P_2_9','Sensor_P_2_10','Sensor_P_2_11','Sensor_P_2_12',\n",
    "             'Sensor_S_2',\n",
    "             'num_finger','properties'   ]\n",
    "\n",
    "    temp = pd.read_pickle(path) \n",
    "    #temp = pd.read_pickle(\"./pickle/hal.pkl\") \n",
    "    temp1 = pd.DataFrame(temp[:].values,columns=Columns)\n",
    "    Total_dataset=temp1\n",
    "\n",
    "    Flag_col = pd.Index(['Flag'])\n",
    "    For_col = pd.Index(['Force_X','Force_Y','Force_Z'])\n",
    "    Input_P_col = pd.Index(['Input_P_1','Input_P_2'])\n",
    "    TCP_col= pd.Index(['TCP_X','TCP_Y','TCP_Z'])\n",
    "\n",
    "    right_finger_pressure1 = pd.Index([ 'Sensor_P_1_1','Sensor_P_1_2',\n",
    "                                      'Sensor_P_1_3','Sensor_P_1_4',\n",
    "                                      'Sensor_P_1_5','Sensor_P_1_6'])\n",
    "\n",
    "    right_finger_pressure2 = pd.Index([ 'Sensor_P_1_7','Sensor_P_1_8',\n",
    "                               'Sensor_P_1_9','Sensor_P_1_10',\n",
    "                               'Sensor_P_1_11','Sensor_P_1_12'])\n",
    "    right_finger_pressure=right_finger_pressure1.append(right_finger_pressure2)\n",
    "    right_finger_strain = pd.Index([ 'Sensor_S_1' ])\n",
    "\n",
    "    right_finger_sensor=right_finger_pressure.append(right_finger_strain)\n",
    "\n",
    "    left_finger_pressure1 = pd.Index([ 'Sensor_P_2_1','Sensor_P_2_2',\n",
    "                             'Sensor_P_2_3','Sensor_P_2_4',\n",
    "                             'Sensor_P_2_5','Sensor_P_2_6'])\n",
    "\n",
    "    left_finger_pressure2 = pd.Index([ 'Sensor_P_2_7','Sensor_P_2_8',\n",
    "                               'Sensor_P_2_9','Sensor_P_2_10',\n",
    "                               'Sensor_P_2_11','Sensor_P_2_12'])\n",
    "    left_finger_pressure=left_finger_pressure1.append(left_finger_pressure2)\n",
    "\n",
    "    left_finger_strain = pd.Index([ 'Sensor_S_2' ])\n",
    "\n",
    "    left_finger_sensor=left_finger_pressure.append(left_finger_strain)\n",
    "\n",
    "\n",
    "    # Robot signal Calibration\n",
    "    Cal_list_col=[For_col,TCP_col]\n",
    "    Sensor_Cal_list_col=[right_finger_sensor,left_finger_sensor]\n",
    "    for i in range(len(Cal_list_col)):\n",
    "        for j in range(len(Cal_list_col[i])):\n",
    "\n",
    "            col_name=Cal_list_col[i][j]\n",
    "\n",
    "            Total_dataset[col_name]=Total_dataset[col_name]-Total_dataset[col_name][0]\n",
    "\n",
    "    # Sensor signal\n",
    "    Sensor_Cal_list_col=[right_finger_sensor,left_finger_sensor]\n",
    "\n",
    "    for i in range(len(Sensor_Cal_list_col)):\n",
    "        for j in range(len(Sensor_Cal_list_col[i])):\n",
    "\n",
    "            col_name=Sensor_Cal_list_col[i][j]\n",
    "\n",
    "            Total_dataset[col_name]=Total_dataset[col_name]-Total_dataset[col_name][0] +0*j\n",
    "    A=pd.DataFrame(Total_dataset.query('Flag==True ').values,columns=Columns)\n",
    "    return A\n",
    "    '''\n",
    "    for i in range(len(Cal_list_col)):\n",
    "        for j in range(len(Cal_list_col[i])):\n",
    "\n",
    "            col_name=Cal_list_col[i][j]\n",
    "\n",
    "            Total_dataset[col_name]=Total_dataset[col_name]-Total_dataset[col_name][0]\n",
    "\n",
    "    # Sensor signal\n",
    "\n",
    "\n",
    "    for i in range(len(Sensor_Cal_list_col)):\n",
    "        for j in range(len(Sensor_Cal_list_col[i])):\n",
    "\n",
    "            col_name=Sensor_Cal_list_col[i][j]\n",
    "\n",
    "            Total_dataset[col_name]=Total_dataset[col_name]-Total_dataset[col_name][0] +0*j\n",
    "\n",
    "    '''\n",
    "def calibration_function(tar:pd.DataFrame,Cal_list_col:list,Sensor_Cal_list_col:list  ):\n",
    "    for i in range(len(Cal_list_col)):\n",
    "        for j in range(len(Cal_list_col[i])):\n",
    "\n",
    "            col_name=Cal_list_col[i][j]\n",
    "\n",
    "            tar[col_name]=tar[col_name]-tar[col_name][0]\n",
    "\n",
    "    # Sensor signal\n",
    "    Sensor_Cal_list_col=[right_finger_sensor,left_finger_sensor]\n",
    "\n",
    "    for i in range(len(Sensor_Cal_list_col)):\n",
    "        for j in range(len(Sensor_Cal_list_col[i])):\n",
    "\n",
    "            col_name=Sensor_Cal_list_col[i][j]\n",
    "\n",
    "            tar[col_name]=tar[col_name]-tar[col_name][0] +0*j\n",
    "    return tar\n",
    "\n",
    "def rawdataplot(Total_dataset):\n",
    "    TEMP_Total_dataset=Total_dataset\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1)\n",
    "    TEMP_Total_dataset[Flag_col].replace({True: 1, False: 0}).plot(ax=axes[0],grid=True,title='Flag')\n",
    "\n",
    "    TEMP_Total_dataset[For_col].plot(ax=axes[1],grid=True,title='F/T sensor(True value)')\n",
    "    TEMP_Total_dataset[Input_P_col].plot(ax=axes[2],grid=True,title='Input pressure')\n",
    "    TEMP_Total_dataset[TCP_col].plot(ax=axes[3],grid=True,title='TCP')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    TEMP_Total_dataset[right_finger_pressure1].plot(ax=axes[0],legend=None,grid=True,title='Pressure Half 1')\n",
    "    TEMP_Total_dataset[right_finger_pressure2].plot(ax=axes[1],legend=None,grid=True,title='Pressure Half 2')\n",
    "    TEMP_Total_dataset[right_finger_strain].plot(ax=axes[2],grid=True,legend=None,title='Strain sensor')\n",
    "    plt.tight_layout()\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "    TEMP_Total_dataset[left_finger_pressure1].plot(ax=axes[0],legend=None,grid=True,title='Pressure Half 1')\n",
    "    TEMP_Total_dataset[left_finger_pressure2].plot(ax=axes[1],legend=None,grid=True,title='Pressure Half 2')\n",
    "    TEMP_Total_dataset[left_finger_strain].plot(ax=axes[2],grid=True,legend=None,title='Strain sensor')\n",
    "    plt.tight_layout()\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "def plot_prediction(Pred_Values,True_Values,Lim_value, nbins=15,save_flag=False, save_name=None):\n",
    "#def plot_loss_curves(results_bunch: dict[str, list[float]]):\n",
    "    \"\"\"Plots Results\n",
    "\n",
    "    Args: True value, Prediction results ,nbins and save flag and save name\n",
    "        \n",
    "    \"\"\"\n",
    "    xlim =Lim_value\n",
    "    ylim =Lim_value\n",
    "    \n",
    "    \n",
    "    fig,host=plt.subplots(nrows=2, ncols=3,figsize=(15,8))\n",
    "    ax0 = host[0][0].twinx()\n",
    "    \n",
    "    host[0][0].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][0].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][0].plot(utils.extraction(True_Values,0),label='True',color='black')\n",
    "    host[0][0].plot(utils.extraction(Pred_Values,0),label='Predict',color='red',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,0))),len(np.array(utils.extraction(True_Values,0))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,0)) - np.array(utils.extraction(Pred_Values,0))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][0].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('X')\n",
    "    \n",
    "    \n",
    "    ax0 = host[0][1].twinx()\n",
    "    \n",
    "    host[0][1].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][1].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][1].plot(utils.extraction(True_Values,1),label='True',color='black')\n",
    "    host[0][1].plot(utils.extraction(Pred_Values,1),label='Predict',color='blue',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,0))),len(np.array(utils.extraction(True_Values,0))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,1)) - np.array(utils.extraction(Pred_Values,1))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][1].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('Y')\n",
    "    \n",
    "    ax0 = host[0][2].twinx()\n",
    "    \n",
    "    host[0][2].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][2].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][2].plot(utils.extraction(True_Values,2),label='True',color='black')\n",
    "    host[0][2].plot(utils.extraction(Pred_Values,2),label='Predict',color='orange',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,2))),len(np.array(utils.extraction(True_Values,2))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,2)) - np.array(utils.extraction(Pred_Values,2))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('Z')\n",
    "   \n",
    "\n",
    "    \n",
    "    #plt.tight_layout(pad=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    #plt.show()\n",
    "    # For \n",
    "\n",
    "\n",
    "    #plt.figure(figsize=(12,3))\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,0)\n",
    "    x = utils.extraction(True_Values,0)\n",
    "    host[1][0].hist2d(x, y, bins=(nbins, nbins), cmap=plt.cm.jet, range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][0].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][0].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('X')\n",
    "\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,1)\n",
    "    x = utils.extraction(True_Values,1)\n",
    "    host[1][1].hist2d(x, y, bins=(nbins, nbins), cmap=plt.cm.jet, range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][1].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][1].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('Y')\n",
    "\n",
    "\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,2)\n",
    "    x = utils.extraction(True_Values,2)\n",
    "    host[1][2].hist2d(x, y, bins=(nbins, nbins), cmap=plt.cm.jet,range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][2].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][2].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('Z')\n",
    "\n",
    "   \n",
    "    fig.suptitle(save_name)\n",
    "    plt.tight_layout(pad=1)\n",
    "    plt.show()\n",
    "    if save_flag==True:\n",
    "        fig.savefig(save_name)\n",
    "    \n",
    "def plot_loss_curves(results_bunch,single_flag=None,save_flag=False, save_name=None):\n",
    "#def plot_loss_curves(results_bunch: dict[str, list[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "   # Setup a plot \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if single_flag==None:\n",
    "        for i in range(len(results_bunch)):\n",
    "            results=results_bunch[i]\n",
    "\n",
    "            # Get the loss values of the results dictionary (training and test)\n",
    "            loss = results['train_loss']\n",
    "            test_loss = results['test_loss']\n",
    "\n",
    "            # Get the accuracy values of the results dictionary (training and test)\n",
    "\n",
    "\n",
    "            # Figure out how many epochs there were\n",
    "            epochs = range(len(results['train_loss']))\n",
    "\n",
    "\n",
    "\n",
    "            # Plot loss\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, loss, label='con '+str(i))\n",
    "            if i==len(results_bunch)-1:\n",
    "                plt.title('Train_Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.legend()\n",
    "\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, test_loss, label='con '+str(i))\n",
    "            if i==len(results_bunch)-1:\n",
    "                plt.title('Test_Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.legend()\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        results=results_bunch\n",
    "         # Get the loss values of the results dictionary (training and test)\n",
    "        loss = results['train_loss']\n",
    "        test_loss = results['test_loss']\n",
    "\n",
    "        # Get the accuracy values of the results dictionary (training and test)\n",
    "\n",
    "\n",
    "        # Figure out how many epochs there were\n",
    "        epochs = range(len(results['train_loss']))\n",
    "\n",
    "\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, loss, label='con ')\n",
    "\n",
    "        plt.title('Train_Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, test_loss, label='con ')\n",
    "        \n",
    "        plt.title('Test_Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(save_name)\n",
    "    plt.tight_layout(pad=1)\n",
    "    \n",
    "    if save_flag==True:\n",
    "        plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f4d6c-f4c3-47ec-a893-2e9df2b7e591",
   "metadata": {},
   "source": [
    "# 2 . Model defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7500c120",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch norm\n",
    "\n",
    "from torch import nn \n",
    "class B_LSTMModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device,batch_type='1d'):\n",
    "        super(B_LSTMModel_V2, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        \n",
    "        if batch_type=='1d':\n",
    "            self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.batch_norm = nn.BatchNorm2d(hidden_dim)\n",
    "    \n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob,device=device)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.batch_norm(out)\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class B_GRUModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device,batch_type='1d'):\n",
    "        super(B_GRUModel_V2, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob,device=device)\n",
    "        \n",
    "        \n",
    "        if batch_type=='1d':\n",
    "            self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.batch_norm = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.batch_norm(out)\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class B_RNNModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device,batch_type='1d'):\n",
    "        super(B_RNNModel_V2, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob,device=device)\n",
    "        \n",
    "        \n",
    "        if batch_type=='1d':\n",
    "            self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.batch_norm = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.batch_norm(out)\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out    \n",
    "from torch import nn \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim : int,layer_dim:int, hidden_dim:int,output_dim:int, dropout_prob:float,activation_function:torch.nn):\n",
    "        super(MLP, self).__init__()\n",
    "        self.activation_function = activation_function\n",
    "        layers=[nn.Linear(input_dim, hidden_dim),self.activation_function,nn.Dropout(p=dropout_prob)]\n",
    "        for i in range(layer_dim):\n",
    "            layers+=[nn.Linear(hidden_dim,hidden_dim),self.activation_function,nn.Dropout(p=dropout_prob)]\n",
    "            \n",
    "        layers+=[nn.Linear(hidden_dim,output_dim)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return torch.squeeze(out,1)\n",
    "\n",
    "class LSTMModel_V0(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V1, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.Relu=nn.leaky_relu()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.Relu(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "class LSTMModel_V1(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V1, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.Relu=nn.ReLU()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.Relu(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device):\n",
    "        super(LSTMModel_V2, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob,\n",
    "        device=device)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V3, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNNModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(RNNModel_V3, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob,device=device)\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "class RNNModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device):\n",
    "        super(RNNModel_V2, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob,device=device)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "\n",
    "class GRUModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(GRUModel_V3, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out  \n",
    "    \n",
    "class GRUModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device):\n",
    "        super(GRUModel_V2, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob,device=device)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class GRUModel_V0(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(GRUModel_V0, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        #out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out  \n",
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.v.data.normal_(mean=0, std=1. / hidden_size**0.5)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Repeat the hidden state for each time step in the sequence\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate the hidden state and encoder outputs\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # Compute attention scores\n",
    "        energy = energy.permute(0, 2, 1)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.unsqueeze(0).unsqueeze(0).repeat(hidden.size(0), 1, 1)  # (batch_size, 1, hidden_size)\n",
    "        attention_scores = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)\n",
    "\n",
    "        # Calculate the weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.permute(0, 2, 1), encoder_outputs)\n",
    "\n",
    "        return context, attention_weights\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "  # def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "    def __init__(self, input_size : int, hidden_size:int, num_layers:int, output_size:int, dropout_prob:float):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        context, attention_weights = self.attention(h_n[-1], lstm_out)\n",
    "\n",
    "        # Concatenate the context vector with the hidden state\n",
    "        lstm_out = torch.cat((context, h_n[-1]), dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        return output ,attention_weights\n",
    "'''\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "        # Fully connected layer for regression\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = F.softmax(self.attention(torch.cat([lstm_out, h_n.repeat(lstm_out.size(1), 1, 1)], dim=2)), dim=1)\n",
    "        context = torch.bmm(attention_weights.permute(0, 2, 1), lstm_out).squeeze(1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(context)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_dim,output_size,dropout_prob):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop=nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        layers=[]\n",
    "        for i in range(layer_dim):\n",
    "            layers+=[self.drop,nn.Linear(hidden_size,hidden_size),self.relu]\n",
    "        self.extra = nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        a=layer_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.extra(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return torch.squeeze(x,1)\n",
    "    \n",
    "'''\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return torch.squeeze(x,1)\n",
    "'''    \n",
    "\n",
    "class Multi_2_nn(nn.Module):\n",
    "    def __init__(self, input_dim1 : int,input_dim2 : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(Multi_2_nn, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm2 = nn.LSTM(input_dim1, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "    def forward(self, x1,x2):\n",
    "        # LSTM layer\n",
    "        lstm_out1, (h_n1, _) = self.lstm1(x1)\n",
    "        lstm_out2, (h_n2, _) = self.lstm2(x2)\n",
    "        out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1) \n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(out)\n",
    "\n",
    "        return output\n",
    "class Multi_3_nn(nn.Module): # Pressure, stain, Given pressure\n",
    "    def __init__(self, input_dim1 : int,input_dim2 : int,input_dim3 ,hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(Multi_3_nn, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm3 = nn.LSTM(input_dim3, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim*3, output_dim)\n",
    "        \n",
    "    def forward(self, x1,x2,x3):\n",
    "        # LSTM layer\n",
    "        lstm_out1, (h_n1, _) = self.lstm1(x1)\n",
    "        lstm_out2, (h_n2, _) = self.lstm2(x2)\n",
    "        lstm_out3, (h_n3, _) = self.lstm3(x3)\n",
    "        out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1) \n",
    "        out = torch.cat((out, lstm_out3[:, -1, :]), dim=1) \n",
    "        # Fully connected layer\n",
    "        output = self.fc(out)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Multi_5_nn(nn.Module): # Pressure, stain, Given pressure\n",
    "    def __init__(self, input_dim1 : int,input_dim2 : int,input_dim3 :int ,input_dim4 :int,input_dim5 :int,hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(Multi_5_nn, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.lstm1 = nn.LSTM(input_dim1, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm2 = nn.LSTM(input_dim2, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm3 = nn.LSTM(input_dim3, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm4 = nn.LSTM(input_dim4, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        self.lstm5 = nn.LSTM(input_dim5, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim*5, output_dim)\n",
    "        \n",
    "    def forward(self, x1,x2,x3,x4,x5):\n",
    "        # LSTM layer\n",
    "        lstm_out1, (h_n1, _) = self.lstm1(x1)\n",
    "        lstm_out2, (h_n2, _) = self.lstm2(x2)\n",
    "        lstm_out3, (h_n3, _) = self.lstm3(x3)\n",
    "        lstm_out4, (h_n4, _) = self.lstm4(x4)\n",
    "        lstm_out5, (h_n5, _) = self.lstm5(x5)\n",
    "        out = torch.cat((lstm_out1[:, -1, :], lstm_out2[:, -1, :]), dim=1) \n",
    "        out = torch.cat((out, lstm_out3[:, -1, :]), dim=1) \n",
    "        out = torch.cat((out, lstm_out4[:, -1, :]), dim=1) \n",
    "        out = torch.cat((out, lstm_out5[:, -1, :]), dim=1) \n",
    "        # Fully connected layer\n",
    "        output = self.fc(out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119ea42-4ad5-4851-8393-8329629975fc",
   "metadata": {},
   "source": [
    "# 3. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b297968",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flag_col = pd.Index(['Flag'])\n",
    "For_col = pd.Index(['Force_X','Force_Y','Force_Z'])\n",
    "Input_P_col = pd.Index(['Input_P_1','Input_P_2'])\n",
    "TCP_col= pd.Index(['TCP_X','TCP_Y','TCP_Z'])\n",
    "# Right finger\n",
    "right_finger_pressure1 = pd.Index([ 'Sensor_P_1_1','Sensor_P_1_2',\n",
    "                                      'Sensor_P_1_3','Sensor_P_1_4',\n",
    "                                      'Sensor_P_1_5','Sensor_P_1_6'])\n",
    "\n",
    "right_finger_pressure2 = pd.Index([ 'Sensor_P_1_7','Sensor_P_1_8',\n",
    "                                   'Sensor_P_1_9','Sensor_P_1_10',\n",
    "                                   'Sensor_P_1_11','Sensor_P_1_12'])\n",
    "right_finger_pressure= pd.Index([ 'Sensor_P_1_1','Sensor_P_1_2',\n",
    "                                   'Sensor_P_1_3','Sensor_P_1_4',\n",
    "                                   'Sensor_P_1_5','Sensor_P_1_6',\n",
    "                                   'Sensor_P_1_7','Sensor_P_1_8',\n",
    "                                   'Sensor_P_1_9','Sensor_P_1_10',\n",
    "                                   'Sensor_P_1_11','Sensor_P_1_12'])\n",
    "right_finger_strain = pd.Index([ 'Sensor_S_1' ])\n",
    "right_finger_sensor=right_finger_pressure.append(right_finger_strain)\n",
    "\n",
    "# Left finger\n",
    "left_finger_pressure1 = pd.Index([ 'Sensor_P_2_1','Sensor_P_2_2',\n",
    "                                     'Sensor_P_2_3','Sensor_P_2_4',\n",
    "                                     'Sensor_P_2_5','Sensor_P_2_6'])\n",
    "\n",
    "left_finger_pressure2 = pd.Index([ 'Sensor_P_2_7','Sensor_P_2_8',\n",
    "                                   'Sensor_P_2_9','Sensor_P_2_10',\n",
    "                                   'Sensor_P_2_11','Sensor_P_2_12'])\n",
    "left_finger_pressure = pd.Index([ 'Sensor_P_2_1','Sensor_P_2_2',\n",
    "                                 'Sensor_P_2_3','Sensor_P_2_4',\n",
    "                                 'Sensor_P_2_5','Sensor_P_2_6',\n",
    "                                 'Sensor_P_2_7','Sensor_P_2_8',\n",
    "                                 'Sensor_P_2_9','Sensor_P_2_10',\n",
    "                                 'Sensor_P_2_11','Sensor_P_2_12'])\n",
    "left_finger_strain = pd.Index([ 'Sensor_S_2' ])\n",
    "left_finger_sensor=left_finger_pressure.append(left_finger_strain)\n",
    "\n",
    "\n",
    "One_entire_condition=right_finger_sensor.append(left_finger_sensor)\n",
    "total_feature=Input_P_col.append(One_entire_condition)\n",
    "\n",
    "\n",
    "TCP_condition=One_entire_condition.append(TCP_col)\n",
    "Columns=['Flag',\n",
    "         'Force_X','Force_Y','Force_Z','Torque_X','Torque_Y','Torque_Z',\n",
    "         'Input_P_1','Input_P_2',\n",
    "         'TCP_X','TCP_Y','TCP_Z',\n",
    "         'Sensor_P_1_1','Sensor_P_1_2','Sensor_P_1_3','Sensor_P_1_4',\n",
    "         'Sensor_P_1_5','Sensor_P_1_6','Sensor_P_1_7','Sensor_P_1_8',\n",
    "         'Sensor_P_1_9','Sensor_P_1_10','Sensor_P_1_11','Sensor_P_1_12',\n",
    "         'Sensor_S_1',\n",
    "         'Sensor_P_2_1','Sensor_P_2_2','Sensor_P_2_3','Sensor_P_2_4',\n",
    "         'Sensor_P_2_5','Sensor_P_2_6','Sensor_P_2_7','Sensor_P_2_8',\n",
    "         'Sensor_P_2_9','Sensor_P_2_10','Sensor_P_2_11','Sensor_P_2_12',\n",
    "         'Sensor_S_2',\n",
    "         'num_finger','properties'   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a2f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "cir=load_data(\"./pickle/dec_13/cir.pkl\")\n",
    "cv=load_data(\"./pickle/dec_13/cv.pkl\")\n",
    "sqr=load_data(\"./pickle/dec_13/sqr.pkl\")\n",
    "nonc=load_data(\"./pickle/dec_13/nonc.pkl\")\n",
    "\n",
    "Cal_list_col=[For_col,TCP_col]\n",
    "Sensor_Cal_list_col=[right_finger_sensor,left_finger_sensor]\n",
    "# Calibrate each file\n",
    "cir1=calibration_function(pd.DataFrame(cir.query('properties==\"1\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cir2=calibration_function(pd.DataFrame(cir.query('properties==\"2\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cir3=calibration_function(pd.DataFrame(cir.query('properties==\"3\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cir4=calibration_function(pd.DataFrame(cir.query('properties==\"4\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cv1=calibration_function(pd.DataFrame(cv.query('properties==\"1\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cv2=calibration_function(pd.DataFrame(cv.query('properties==\"2\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cv3=calibration_function(pd.DataFrame(cv.query('properties==\"3\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "cv4=calibration_function(pd.DataFrame(cv.query('properties==\"4\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "\n",
    "sqr1=calibration_function(pd.DataFrame(sqr.query('properties==\"1\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "sqr2=calibration_function(pd.DataFrame(sqr.query('properties==\"2\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "sqr3=calibration_function(pd.DataFrame(sqr.query('properties==\"3\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "sqr4=calibration_function(pd.DataFrame(sqr.query('properties==\"4\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "\n",
    "nonc1=calibration_function(pd.DataFrame(nonc.query('properties==\"1\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "nonc2=calibration_function(pd.DataFrame(nonc.query('properties==\"2\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "nonc3=calibration_function(pd.DataFrame(nonc.query('properties==\"3\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "nonc4=calibration_function(pd.DataFrame(nonc.query('properties==\"4\"  ').values,columns=Columns),Cal_list_col,Sensor_Cal_list_col)\n",
    "\n",
    "# total data set\n",
    "total=pd.concat([cir1, nonc1,cv1,sqr1,cir2,nonc2,cv2,sqr2,cir3,sqr3,cv3,nonc3,cir4,cv4,sqr4,nonc4],ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e246b2-5e53-4149-8b49-11b34d873854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_address =\"./pickle/dec_13/totaldata_set.pkl\" # change the pickle address\n",
    "total.to_pickle(pickle_address)  # save the pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c8bba-bbad-409a-8067-205753366479",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Training\n",
    "- training different conditions of the model\n",
    "- tunable parameter : number of layer, sequential length, num_epochs, batchsize, model, hidden node, learning late, dropout_prob.\n",
    "### save file \n",
    "- model  : address \"models/today_date/model\"\n",
    "- scaler : address \"scaler/today_date/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1111e6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d821c9cad9044ec4b4a8e2b3cd27e6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 2.3731 | test_loss: 2.7542 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#torchinfo.summary(model,(5,5,5))\u001b[39;00m\n\u001b[1;32m    169\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m--> 170\u001b[0m results\u001b[38;5;241m=\u001b[39m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m             \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m             \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime taken : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/janghun/External_force-main/engine.py:163\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 163\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    169\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    170\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    171\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[0;32m~/janghun/External_force-main/engine.py:37\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Loop through data loader data batches\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Send data to target device\u001b[39;00m\n\u001b[1;32m     39\u001b[0m       X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m       \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/janghun/External_force-main/utils.py:130\u001b[0m, in \u001b[0;36mData.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     i_start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "duration_time=[]\n",
    "loss_results=[]\n",
    "final_results=[]\n",
    "result_rmse=[]\n",
    "\n",
    "today_date='dec_23' # folder name\n",
    "dscrpt='' # any description\n",
    "\n",
    "\n",
    "\n",
    "# Hyper parameters\n",
    "LAYER_DIM1=[1,2,5,10] # \n",
    "Sequential_L1=[1,2,5,10] #\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS1 = [1,2,5,10]\n",
    "LEARNING_RATE = 0.001\n",
    "Dropout_prob=0.2\n",
    "model_name=['mpl','lstm','gru','rnn']\n",
    "\n",
    "batch=['1d','no']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_d= pd.DataFrame(total.query('properties<\"4\"  ').values,columns=Columns)\n",
    "test_d= pd.DataFrame(total.query('properties==\"4\" ').values,columns=Columns)\n",
    "for m in range(len(batch)):\n",
    "    dscrpt=batch[m]\n",
    "    \n",
    "    for i in range(len(LAYER_DIM1)):\n",
    "        for j in range(len(Sequential_L1)):\n",
    "            for k in range(len(HIDDEN_UNITS1)):\n",
    "                for l in range(len(model_name)):\n",
    "                    HIDDEN_UNITS=HIDDEN_UNITS1[k]\n",
    "                    LAYER_DIM=LAYER_DIM1[i]\n",
    "                    Sequential_L=Sequential_L1[j]\n",
    "                    name=model_name[l]+today_date+\"LD_\"+str(LAYER_DIM)+\"SQ_\"+str(Sequential_L)+\"HU_\"+str(HIDDEN_UNITS)+dscrpt\n",
    "                    prediction_figure_name='./figure/'+today_date+'/predict_'+name+'.png'\n",
    "                    loss_function_figure_name='./figure/'+today_date+'/loss_'+name+'.png'\n",
    "\n",
    "\n",
    "\n",
    "                    Y_tr=np.array(train_d.filter(items=For_col))\n",
    "                    X_tr=np.array(train_d.filter(items=total_feature)) ## important part\n",
    "\n",
    "                    #X_tr_scaler = sklearn.preprocessing.StandardScaler()\n",
    "                    #Y_tr_scaler = sklearn.preprocessing.StandardScaler()\n",
    "                    X_tr_scaler = sklearn.preprocessing.RobustScaler()\n",
    "                    Y_tr_scaler = sklearn.preprocessing.RobustScaler()\n",
    "                    #X_tr_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "                    #Y_tr_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "                    #X_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "                    #Y_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "                    Y_te=np.array(test_d.filter(items=For_col))\n",
    "                    X_te=np.array(test_d.filter(items=total_feature)) ## important part\n",
    "\n",
    "\n",
    "\n",
    "                    X_test=torch.FloatTensor(X_tr_scaler.fit_transform(X_te))\n",
    "                    Y_test=torch.FloatTensor(Y_tr_scaler.fit_transform(Y_te))\n",
    "\n",
    "\n",
    "\n",
    "                    X_train=torch.FloatTensor(X_tr_scaler.fit_transform(X_tr))\n",
    "                    Y_train=torch.FloatTensor(Y_tr_scaler.fit_transform(Y_tr))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    train_dataset=DataLoader(utils.Data(X_train,Y_train,Sequential_L),batch_size=BATCH_SIZE,shuffle=False)\n",
    "                    test_dataset=DataLoader(utils.Data(X_test,Y_test,Sequential_L),batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "\n",
    "                    Input_dim=len(X_train[0])\n",
    "                    Output_dim=len(Y_train[0])\n",
    "                    if dscrpt == '1d':\n",
    "                        if model_name[l]=='mpl':\n",
    "                            if Sequential_L!=1:\n",
    "                                continue\n",
    "                            model = SimpleNN(input_size=Input_dim,\n",
    "                                        hidden_size=HIDDEN_UNITS,\n",
    "                                        layer_dim = LAYER_DIM,\n",
    "                                        output_size=Output_dim,\n",
    "                                        dropout_prob=0.2  \n",
    "                                        )\n",
    "                            '''\n",
    "                            model = MLP(input_dim=Input_dim,\n",
    "                                        layer_dim=LAYER_DIM, \n",
    "                                        hidden_dim=HIDDEN_UNITS,\n",
    "                                        output_dim=Output_dim, \n",
    "                                        dropout_prob=0.2,\n",
    "                                        activation_function=nn.ReLU())'''\n",
    "                        elif model_name[l]=='lstm':\n",
    "                            model = B_LSTMModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device,\n",
    "                                                batch_type=batch[m])\n",
    "                        elif model_name[l]=='gru':\n",
    "                            model = B_GRUModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device,\n",
    "                                                batch_type=batch[m])\n",
    "                        elif model_name[l]=='rnn':\n",
    "                            model = B_RNNModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device,\n",
    "                                                batch_type=batch[m])\n",
    "                    else: \n",
    "                        if model_name[l]=='mpl':\n",
    "                            if Sequential_L!=1:\n",
    "                                continue\n",
    "                            model = SimpleNN(input_size=Input_dim,\n",
    "                                        hidden_size=HIDDEN_UNITS,\n",
    "                                        layer_dim = LAYER_DIM,\n",
    "                                        output_size=Output_dim,\n",
    "                                        dropout_prob=0.2  \n",
    "                                        )\n",
    "\n",
    "                            #model = MLP(input_dim=Input_dim,\n",
    "                             #           layer_dim=LAYER_DIM, \n",
    "                              #          hidden_dim=HIDDEN_UNITS,\n",
    "                               #         output_dim=Output_dim, \n",
    "                                #        dropout_prob=0.2,\n",
    "                                 #       activation_function=nn.ReLU())\n",
    "                        elif model_name[l]=='lstm':\n",
    "                            model = LSTMModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device)\n",
    "                        elif model_name[l]=='gru':\n",
    "                            model = GRUModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device)\n",
    "                        elif model_name[l]=='rnn':\n",
    "                            model = RNNModel_V2(    input_dim = Input_dim,\n",
    "                                                hidden_dim=HIDDEN_UNITS,\n",
    "                                                layer_dim=LAYER_DIM,\n",
    "                                                output_dim=Output_dim,\n",
    "                                                dropout_prob=0.2,device=device)\n",
    "\n",
    "                    \n",
    "                    model.to(device)\n",
    "\n",
    "\n",
    "                    # Set loss and optimizer\n",
    "                    loss_fn = torch.nn.MSELoss() ## change\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), ## change\n",
    "                                                 lr=LEARNING_RATE)\n",
    "                    # Train\n",
    "                    #torchinfo.summary(model,(5,5,5))\n",
    "                    start_time = timer()\n",
    "                    results=engine.train(model=model,\n",
    "                                 train_dataloader=train_dataset,\n",
    "                                 test_dataloader=test_dataset,\n",
    "                                 loss_fn=loss_fn,\n",
    "                                 optimizer=optimizer,\n",
    "                                 epochs=NUM_EPOCHS,\n",
    "                                 device=device)\n",
    "                    end_time = timer()\n",
    "\n",
    "                    print(f'Time taken : {end_time-start_time}')\n",
    "                    loss_results.append(results)\n",
    "                    Pred_Values=Y_tr_scaler.inverse_transform(utils.predict(model,test_dataset,device).to('cpu'))\n",
    "                    True_Values=Y_tr_scaler.inverse_transform(Y_test)\n",
    "                    result_rmse.append( rmse(Pred_Values,True_Values))\n",
    "                    utils.save_model(model=model,\n",
    "                                     target_dir=\"models/\"+today_date,\n",
    "                                     model_name=name+\".pth\")\n",
    "                    plot_prediction(Pred_Values,True_Values,2,save_flag=True,save_name=prediction_figure_name )\n",
    "                    plot_loss_curves(results,single_flag=True,save_flag=True, save_name=loss_function_figure_name)\n",
    "                    joblib.dump(X_tr_scaler, \"scaler/\"+today_date+\"/X\"+name+'.save') \n",
    "                    joblib.dump(Y_tr_scaler, \"scaler/\"+today_date+\"/Y\"+name+'.save') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f365a45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float,device):\n",
    "        super(LSTMModel_V2, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                            hidden_dim, \n",
    "                            layer_dim, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout_prob,\n",
    "                            device=device)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim,device=x.device).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9e5ef3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))[0][0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8267a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LSTMModel_V2(input_dim = 28,\n",
    "                             hidden_dim=10,\n",
    "                             layer_dim=2,\n",
    "                             output_dim=3,\n",
    "                             dropout_prob=0.2,device=device)\n",
    "model.load_state_dict(torch.load(\"./models/dec_23/lstmdec_23LD_2SQ_10HU_10no.pth\",map_location=torch.device(device)))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    prediction=model(torch.unsqueeze(next(iter(train_dataset))[0][0].to(device),0))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57b0510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0427,  0.0010,  0.0860]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb4baff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 28])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next(iter(train_dataset))[0][0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63f41191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel_V2(\n",
       "  (lstm): LSTM(28, 10, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (act_F): Tanh()\n",
       "  (fc): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbd377a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m prediction\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 30\u001b[0m, in \u001b[0;36mLSTMModel_V2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initializing cell state for first input with zeros\u001b[39;00m\n\u001b[1;32m     27\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dim, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim,device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m---> 30\u001b[0m out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_F(out)\n\u001b[1;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:803\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    801\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 803\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    804\u001b[0m         hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93c9d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=DataLoader(utils.Data(X_train,Y_train,10),batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57dc759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642],\n",
       "        [-0.3333, -0.3333, -0.1797, -0.4001,  0.0067,  0.2003,  0.0163,  0.0031,\n",
       "          0.0080,  0.0277,  0.2813, -0.3333, -0.5060, -0.1244, -0.2629,  0.7127,\n",
       "          0.5044,  0.3363,  0.1997,  0.2176,  0.2026,  0.2207,  0.2238,  0.1674,\n",
       "          0.2635,  0.3248,  0.1898, -0.2642]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
