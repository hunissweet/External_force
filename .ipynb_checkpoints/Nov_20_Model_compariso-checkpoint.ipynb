{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9accd6-d6c9-4e62-be8c-67f03bc22c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import LogNorm\n",
    "import sklearn.preprocessing\n",
    "#from directory_tree import display_tree\n",
    "# Customed Library\n",
    "import engine ,model_builder,utils\n",
    "import numpy as np\n",
    "#import torchinfo\n",
    "from timeit import default_timer as timer \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "#display_tree('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7eb7cf-8d50-42e6-83b6-7a7a84f2ce52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_prediction(Pred_Values,True_Values,Lim_value):\n",
    "#def plot_loss_curves(results_bunch: dict[str, list[float]]):\n",
    "    \"\"\"Plots Results\n",
    "\n",
    "    Args: True value, Prediction results \n",
    "        \n",
    "    \"\"\"\n",
    "    xlim =Lim_value\n",
    "    ylim =Lim_value\n",
    "    \n",
    "    \n",
    "    fig,host=plt.subplots(nrows=2, ncols=3,figsize=(15,8))\n",
    "    ax0 = host[0][0].twinx()\n",
    "    \n",
    "    host[0][0].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][0].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][0].plot(utils.extraction(True_Values,0),label='True',color='black')\n",
    "    host[0][0].plot(utils.extraction(Pred_Values,0),label='Predict',color='red',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,0))),len(np.array(utils.extraction(True_Values,0))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,0)) - np.array(utils.extraction(Pred_Values,0))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][0].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('X')\n",
    "    \n",
    "    \n",
    "    ax0 = host[0][1].twinx()\n",
    "    \n",
    "    host[0][1].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][1].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][1].plot(utils.extraction(True_Values,1),label='True',color='black')\n",
    "    host[0][1].plot(utils.extraction(Pred_Values,1),label='Predict',color='blue',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,0))),len(np.array(utils.extraction(True_Values,0))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,1)) - np.array(utils.extraction(Pred_Values,1))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][1].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('Y')\n",
    "    \n",
    "    ax0 = host[0][2].twinx()\n",
    "    \n",
    "    host[0][2].set_ylim(-7, 8)\n",
    "    ax0.set_ylim(-0, 10)\n",
    "    \n",
    "    host[0][2].set_ylabel(\"Force[N]\")\n",
    "    ax0.set_ylabel(\"RMSE[N]\")\n",
    "\n",
    "    \n",
    "    host[0][2].plot(utils.extraction(True_Values,2),label='True',color='black')\n",
    "    host[0][2].plot(utils.extraction(Pred_Values,2),label='Predict',color='orange',linestyle =\"--\")\n",
    "    \n",
    "    A0=np.linspace(0,len(np.array(utils.extraction(True_Values,2))),len(np.array(utils.extraction(True_Values,2))))\n",
    "    \n",
    "    ax0.fill_between(A0,0, np.absolute(np.array(utils.extraction(True_Values,2)) - np.array(utils.extraction(Pred_Values,2))),  label='RMSE',alpha=.3)\n",
    "    \n",
    "    # ask matplotlib for the plotted objects and their labels\n",
    "    lines, labels = host[0][2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax0.get_legend_handles_labels()\n",
    "    ax0.legend(lines + lines2, labels + labels2, loc=0)\n",
    "    plt.title('Z')\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    #plt.tight_layout(pad=3)\n",
    "    \n",
    "\n",
    "    \n",
    "    #plt.show()\n",
    "    # For \n",
    "\n",
    "\n",
    "    #plt.figure(figsize=(12,3))\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,0)\n",
    "    x = utils.extraction(True_Values,0)\n",
    "    host[1][0].hist2d(x, y, bins=(15, 15), cmap=plt.cm.jet, range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][0].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][0].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('X')\n",
    "\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,1)\n",
    "    x = utils.extraction(True_Values,1)\n",
    "    host[1][1].hist2d(x, y, bins=(15, 15), cmap=plt.cm.jet, range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][1].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][1].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('Y')\n",
    "\n",
    "\n",
    "    \n",
    "    y = utils.extraction(Pred_Values,2)\n",
    "    x = utils.extraction(True_Values,2)\n",
    "    host[1][2].hist2d(x, y, bins=(15, 15), cmap=plt.cm.jet,range=[[-xlim, xlim], [-ylim, ylim]])\n",
    "    host[1][2].set_xlabel('Ground Truth Force Magnitude (N)')\n",
    "    host[1][2].set_ylabel('Predicted Force Magnitude (N)')\n",
    "    plt.title('Z')\n",
    "\n",
    "    plt.tight_layout(pad=1)\n",
    "\n",
    "    plt.show()\n",
    "def plot_loss_curves(results_bunch,single_flag=None):\n",
    "#def plot_loss_curves(results_bunch: dict[str, list[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "   # Setup a plot \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if single_flag==None:\n",
    "        for i in range(len(results_bunch)):\n",
    "            results=results_bunch[i]\n",
    "\n",
    "            # Get the loss values of the results dictionary (training and test)\n",
    "            loss = results['train_loss']\n",
    "            test_loss = results['test_loss']\n",
    "\n",
    "            # Get the accuracy values of the results dictionary (training and test)\n",
    "\n",
    "\n",
    "            # Figure out how many epochs there were\n",
    "            epochs = range(len(results['train_loss']))\n",
    "\n",
    "\n",
    "\n",
    "            # Plot loss\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs, loss, label='con '+str(i))\n",
    "            if i==len(results_bunch)-1:\n",
    "                plt.title('Train_Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.legend()\n",
    "\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs, test_loss, label='con '+str(i))\n",
    "            if i==len(results_bunch)-1:\n",
    "                plt.title('Test_Loss')\n",
    "                plt.xlabel('Epochs')\n",
    "                plt.legend()\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        results=results_bunch\n",
    "         # Get the loss values of the results dictionary (training and test)\n",
    "        loss = results['train_loss']\n",
    "        test_loss = results['test_loss']\n",
    "\n",
    "        # Get the accuracy values of the results dictionary (training and test)\n",
    "\n",
    "\n",
    "        # Figure out how many epochs there were\n",
    "        epochs = range(len(results['train_loss']))\n",
    "\n",
    "\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, loss, label='con ')\n",
    "\n",
    "        plt.title('Train_Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, test_loss, label='con ')\n",
    "        \n",
    "        plt.title('Test_Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dddc29-3bf1-4bad-8b28-2a84c29d53c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel_V0(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V1, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.Relu=nn.leaky_relu()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.Relu(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "class LSTMModel_V1(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V1, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.Relu=nn.ReLU()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.Relu(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V2, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(LSTMModel_V3, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.act_F(out)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RNNModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(RNNModel_V3, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class GRUModel_V3(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(GRUModel_V3, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Sigmoid()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out  \n",
    "    \n",
    "class GRUModel_V2(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(GRUModel_V2, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class GRUModel_V0(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim:int, layer_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(GRUModel_V0, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim,num_layers=layer_dim, batch_first=True,dropout=dropout_prob)\n",
    "        \n",
    "        self.act_F=nn.Tanh()\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: input tensor of shape (batch_size, sequence_length, input_size)\n",
    "        # h0: initial hidden state (optional)\n",
    "\n",
    "        # RNN layer\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        #out = self.act_F(out)\n",
    "        # Select the last time step's output\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out  \n",
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.v.data.normal_(mean=0, std=1. / hidden_size**0.5)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Repeat the hidden state for each time step in the sequence\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate the hidden state and encoder outputs\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # Compute attention scores\n",
    "        energy = energy.permute(0, 2, 1)  # (batch_size, hidden_size, seq_len)\n",
    "        v = self.v.unsqueeze(0).unsqueeze(0).repeat(hidden.size(0), 1, 1)  # (batch_size, 1, hidden_size)\n",
    "        attention_scores = torch.bmm(v, energy).squeeze(1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)\n",
    "\n",
    "        # Calculate the weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.permute(0, 2, 1), encoder_outputs)\n",
    "\n",
    "        return context, attention_weights\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "  # def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "    def __init__(self, input_size : int, hidden_size:int, num_layers:int, output_size:int, dropout_prob:float):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        context, attention_weights = self.attention(h_n[-1], lstm_out)\n",
    "\n",
    "        # Concatenate the context vector with the hidden state\n",
    "        lstm_out = torch.cat((context, h_n[-1]), dim=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        return output ,attention_weights\n",
    "'''\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "        # Fully connected layer for regression\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_weights = F.softmax(self.attention(torch.cat([lstm_out, h_n.repeat(lstm_out.size(1), 1, 1)], dim=2)), dim=1)\n",
    "        context = torch.bmm(attention_weights.permute(0, 2, 1), lstm_out).squeeze(1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.fc(context)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return torch.squeeze(x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044247c-bb9c-402a-aeb6-8a93bc23dba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c6894-8c81-44f0-bfc0-13205fb102b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c954914-a90d-4a80-a2c0-e645e2b3b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
